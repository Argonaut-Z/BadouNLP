# 主流大模型结构代码对比分析

我将对比分析几种主流大语言模型的结构代码差异，包括Transformer原始结构、GPT系列、BERT、LLaMA和PaLM等模型的架构特点。

## 1. Transformer基础结构

原始Transformer论文中的编码器-解码器结构：

```python
class Transformer(nn.Module):
    def __init__(self, src_vocab, tgt_vocab, d_model, N, heads, dropout):
        super().__init__()
        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)
        self.decoder = Decoder(tgt_vocab, d_model, N, heads, dropout)
        self.out = nn.Linear(d_model, tgt_vocab)

    def forward(self, src, tgt, src_mask, tgt_mask):
        e_outputs = self.encoder(src, src_mask)
        d_output = self.decoder(tgt, e_outputs, src_mask, tgt_mask)
        return self.out(d_output)
```

关键组件：
- 多头注意力机制
- 位置前馈网络
- 残差连接和层归一化
- 编码器-解码器注意力

## 2. GPT系列 (单向Transformer解码器)

GPT模型仅使用Transformer的解码器部分，采用单向注意力：

```python
class GPT(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads, dropout):
        super().__init__()
        self.embed = Embedder(vocab_size, d_model)
        self.pe = PositionalEncoder(d_model, dropout=dropout)
        self.layers = nn.ModuleList([DecoderLayer(d_model, heads, dropout) for _ in range(N)])
        self.norm = nn.LayerNorm(d_model)
        self.out = nn.Linear(d_model, vocab_size)

    def forward(self, src, mask):
        x = self.embed(src)
        x = self.pe(x)
        for layer in self.layers:
            x = layer(x, mask)
        x = self.norm(x)
        return self.out(x)
```

特点：
- 仅解码器结构
- 因果注意力掩码(防止信息泄漏)
- 自回归生成方式
- GPT-3增加到96层，d_model=12288

## 3. BERT (双向Transformer编码器)

BERT使用Transformer编码器部分，采用双向注意力：

```python
class BERT(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads, dropout):
        super().__init__()
        self.embed = BERTEmbedding(vocab_size, d_model)
        self.layers = nn.ModuleList([EncoderLayer(d_model, heads, dropout) for _ in range(N)])
        self.norm = nn.LayerNorm(d_model)

    def forward(self, x, mask):
        x = self.embed(x)
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
```

特点：
- 仅编码器结构
- 全向注意力(无因果掩码)
- 预训练任务：MLM和NSP
- BERT-large有24层，d_model=1024

## 4. LLaMA (改进的GPT结构)

Meta的LLaMA在GPT结构上做了优化：

```python
class LLaMA(nn.Module):
    def __init__(self, params):
        super().__init__()
        self.params = params
        self.vocab_size = params.vocab_size
        self.n_layers = params.n_layers
        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)
        self.layers = nn.ModuleList()
        for _ in range(params.n_layers):
            self.layers.append(TransformerBlock(params))
        self.norm = RMSNorm(params.dim)
        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)

    def forward(self, tokens, start_pos=0):
        h = self.tok_embeddings(tokens)
        for layer in self.layers:
            h = layer(h, start_pos)
        h = self.norm(h)
        return self.output(h)
```

改进点：
- 使用RMSNorm代替LayerNorm
- SwiGLU激活函数
- 旋转位置编码(RoPE)
- 移除偏置项(只保留注意力中的QKV偏置)
- 65B参数版本使用80层，d_model=8192

## 5. PaLM (谷歌的Pathways架构)

PaLM采用并行设计：

```python
class ParallelBlock(nn.Module):
    def __init__(self, config):
        self.attention = Attention(config)
        self.mlp = MLP(config)
        self.input_layernorm = nn.LayerNorm(config.hidden_size)
        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size)

    def forward(self, x):
        # 并行计算注意力和MLP
        attn_out = self.attention(self.input_layernorm(x))
        mlp_out = self.mlp(self.post_attention_layernorm(x))
        return x + attn_out + mlp_out
```

特点：
- 并行计算注意力和MLP
- 共享输入输出投影
- 使用SwiGLU激活
- 540B参数版本使用118层，d_model=18432

## 结构对比表

| 模型 | 结构类型 | 层归一化 | 位置编码 | 注意力类型 | 典型配置 |
|------|---------|---------|---------|-----------|---------|
| Transformer | 编码器-解码器 | LayerNorm | 正弦 | 全向/因果 | base: 6层 |
| GPT-3 | 解码器 | LayerNorm | 学习式 | 因果 | 96层,d=12288 |
| BERT | 编码器 | LayerNorm | 学习式 | 全向 | large: 24层 |
| LLaMA | 解码器 | RMSNorm | RoPE | 因果 | 65B: 80层 |
| PaLM | 解码器 | LayerNorm | RoPE | 因果+并行 | 540B: 118层 |

## 关键差异总结

1. **归一化方式**：
   - 传统：LayerNorm
   - LLaMA：RMSNorm(计算更简单)

2. **位置编码**：
   - 原始：正弦/学习式
   - 现代：RoPE(旋转位置编码)

3. **注意力机制**：
   - GPT类：因果掩码
   - BERT类：全向注意力
   - PaLM：并行注意力+MLP

4. **前馈网络**：
   - 原始：ReLU+两层线性
   - 现代：SwiGLU/GEGLU

5. **规模扩展**：
   - 早期模型：<1B参数
   - 现代大模型：100B+参数
   - 最新趋势：混合专家(MoE)结构

这些结构上的改进共同推动了模型性能的提升，同时也在计算效率和训练稳定性方面做出了优化。

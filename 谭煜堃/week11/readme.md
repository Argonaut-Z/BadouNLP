对比“chatglm1、chatglm2、deepseek-v3、llama2、qwen-7B、kimi”这几款对比几款国内模型在
“位置编码(ROPE还是Alibi）、
      transformer结构（串行还是平行）、
      多头机制（传统方式还是multi query）、
      ff层设计（传统方式还是gated形式）、
      归一化层选择（RMSnorm还是 Layernorm）、
      激活函数（SILU还是gelu还是relu）、
      各层矩阵是否使用了bias”
几个维度的的结构差异
